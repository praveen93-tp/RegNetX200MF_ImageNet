# -*- coding: utf-8 -*-
"""RegNetMain.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1L47h01ltQc6l3Y0fEyWk59lGTqrVzT7E
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import argparse


# torch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.autograd import Function
import numpy as np
# torch utils
import torchvision
import torchvision.transforms as transforms

# additional libraries
import os
import urllib.request
import zipfile
import time
import math
import numpy             as np
import matplotlib.pyplot as plt

################################################################################
#
# PARAMETERS
#
################################################################################
# data
DATA_DIR_1        = 'data'
DATA_DIR_2        = 'data/imagenet64'
DATA_DIR_TRAIN    = 'data/imagenet64/train'
DATA_DIR_TEST     = 'data/imagenet64/val'
DATA_FILE_TRAIN_1 = 'Train1.zip'
DATA_FILE_TRAIN_2 = 'Train2.zip'
DATA_FILE_TRAIN_3 = 'Train3.zip'
DATA_FILE_TRAIN_4 = 'Train4.zip'
DATA_FILE_TRAIN_5 = 'Train5.zip'
DATA_FILE_TEST_1  = 'Val1.zip'
DATA_URL_TRAIN_1  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train1.zip'
DATA_URL_TRAIN_2  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train2.zip'
DATA_URL_TRAIN_3  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train3.zip'
DATA_URL_TRAIN_4  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train4.zip'
DATA_URL_TRAIN_5  = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Train5.zip'
DATA_URL_TEST_1   = 'https://github.com/arthurredfern/UT-Dallas-CS-6301-CNNs/raw/master/Data/Val1.zip'
DATA_BATCH_SIZE   = 512
DATA_NUM_WORKERS  = 4
DATA_NUM_CHANNELS = 3
DATA_NUM_CLASSES  = 100
DATA_RESIZE       = 64
DATA_CROP         = 56
DATA_MEAN         = (0.485, 0.456, 0.406)
DATA_STD_DEV      = (0.229, 0.224, 0.225)



################################################################################
#
# DATA
#
################################################################################

# create a local directory structure for data storage
if (os.path.exists(DATA_DIR_1) == False):
    os.mkdir(DATA_DIR_1)
if (os.path.exists(DATA_DIR_2) == False):
    os.mkdir(DATA_DIR_2)
if (os.path.exists(DATA_DIR_TRAIN) == False):
    os.mkdir(DATA_DIR_TRAIN)
if (os.path.exists(DATA_DIR_TEST) == False):
    os.mkdir(DATA_DIR_TEST)

# download data
if (os.path.exists(DATA_FILE_TRAIN_1) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_1, DATA_FILE_TRAIN_1)
if (os.path.exists(DATA_FILE_TRAIN_2) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_2, DATA_FILE_TRAIN_2)
if (os.path.exists(DATA_FILE_TRAIN_3) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_3, DATA_FILE_TRAIN_3)
if (os.path.exists(DATA_FILE_TRAIN_4) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_4, DATA_FILE_TRAIN_4)
if (os.path.exists(DATA_FILE_TRAIN_5) == False):
    urllib.request.urlretrieve(DATA_URL_TRAIN_5, DATA_FILE_TRAIN_5)
if (os.path.exists(DATA_FILE_TEST_1) == False):
    urllib.request.urlretrieve(DATA_URL_TEST_1, DATA_FILE_TEST_1)

# extract data
with zipfile.ZipFile(DATA_FILE_TRAIN_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_2, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_3, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_4, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TRAIN_5, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TRAIN)
with zipfile.ZipFile(DATA_FILE_TEST_1, 'r') as zip_ref:
    zip_ref.extractall(DATA_DIR_TEST)



# transforms
transform_train = transforms.Compose([transforms.RandomResizedCrop(DATA_CROP), transforms.RandomHorizontalFlip(p=0.5), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])
transform_test  = transforms.Compose([transforms.Resize(DATA_RESIZE), transforms.CenterCrop(DATA_CROP), transforms.ToTensor(), transforms.Normalize(DATA_MEAN, DATA_STD_DEV)])

# data sets
dataset_train = torchvision.datasets.ImageFolder(DATA_DIR_TRAIN, transform=transform_train)
dataset_test  = torchvision.datasets.ImageFolder(DATA_DIR_TEST,  transform=transform_test)

# data loader
dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=DATA_BATCH_SIZE, shuffle=True,  num_workers=DATA_NUM_WORKERS, pin_memory=True, drop_last=True)
dataloader_test  = torch.utils.data.DataLoader(dataset_test,  batch_size=DATA_BATCH_SIZE, shuffle=False, num_workers=DATA_NUM_WORKERS, pin_memory=True, drop_last=True)


#X_Block
class X_Block(nn.Module):
  def __init__(self, in_filters, out_filters, bottleneck_ratio, group_size, stride=1, se_ratio=1):
    super(X_Block, self).__init__()
    #1x1 Bottleneck Convolution Block
    bottleneck_filters = in_filters // bottleneck_ratio
    self.conv1_1x1 = nn.Conv2d(in_filters, bottleneck_filters, kernel_size=1, bias=False) 
    self.bn1 = nn.BatchNorm2d(bottleneck_filters)
    #3x3 Convolution Block with Group Convolutions
    num_groups = bottleneck_filters // group_size
    self.conv2_3x3 = nn.Conv2d(bottleneck_filters, bottleneck_filters, kernel_size=3, stride=stride, padding=1, groups=num_groups, bias=False)
    self.bn2 = nn.BatchNorm2d(bottleneck_filters)
    #SE Layer// extra implemented..
    if se_ratio<1:
      se_channels = int(bottleneck_filters*se_ratio)
      self.se_module = nn.Sequential(
          nn.AdaptiveAvgPool2d(output_size=1),
          nn.Conv2d(bottleneck_filters, se_channels, kernel_size=1, bias=True),
          nn.ReLU(),
          nn.Conv2d(se_channels, bottleneck_filters, kernel_size=1, bias=True),
          nn.Sigmoid(),
          )
    else:
      self.se_module = None
    #Downsample
    if stride != 1 or in_filters != out_filters:
      self.downsample = nn.Sequential(
          nn.Conv2d(in_filters, out_filters, kernel_size=1, stride=stride, bias=False),
          nn.BatchNorm2d(out_filters)
          )
    else:
      self.downsample = None
    #1x1 Convolution Block
    self.conv3_1x1 = nn.Conv2d(bottleneck_filters, out_filters, kernel_size=1, bias=False)
    self.bn3 = nn.BatchNorm2d(out_filters)

  def forward(self, x):
    residual = x
    out = F.relu(self.bn1(self.conv1_1x1(x)))
    out = F.relu(self.bn2(self.conv2_3x3(out)))
    if self.se_module is not None:
      out = out * self.se_module(out)
    out = self.bn3(self.conv3_1x1(out))
    if self.downsample is not None:
      residual = self.downsample(x)
    out += residual
    out = F.relu(out)
    return out

#Head block
class Head(nn.Module):
  def __init__(self, in_filters, classes):
    super(Head, self).__init__()
    self.avgpool = nn.AdaptiveAvgPool2d(output_size=1)
    self.fc = nn.Linear(in_filters, classes)
  def forward(self, x):
      out = self.avgpool(x)
      out = torch.flatten(out, 1)
      out = self.fc(out)
      return out

#Stem block
class Stem(nn.Module):
  def __init__(self, out_filters, in_filters=3):
    super(Stem, self).__init__()
    self.conv3x3 = nn.Conv2d(in_filters, out_filters, kernel_size=3, stride=1, padding=1, bias=False)
    self.bn = nn.BatchNorm2d(out_filters)
  def forward(self, x):
    return F.relu(self.bn(self.conv3x3(x)))
 
class Stage(nn.Module):
  def __init__(self, in_filters, depth, width, bottleneck_ratio, group_size, se_ratio,stride_flag):
    super(Stage, self).__init__()
    self.layers = []
    #Total bottleneck blocks in a layer=d
    for index in range(depth):
      if stride_flag:
        stride=1
      else:
        stride = 2 if index == 0 else 1
      bottleneck = X_Block(in_filters, width, bottleneck_ratio, group_size, stride, se_ratio)
      self.layers.append(bottleneck)
      in_filters = width
    self.layers = nn.Sequential(*self.layers)
  def forward(self, x):
    out = self.layers(x)
    return out
 
class RegNet(nn.Module):
  def __init__(self, network_depth, initial_width, in_filters, number_layer, slope, quantized_param, bottleneck_ratio, group_width, se_ratio , classes=100):
    super(RegNet, self).__init__()
    #Model paramater calculation
    param_width = initial_width + slope * np.arange(network_depth) # Equation 1
    param_block = np.log(param_width / initial_width) / np.log(quantized_param) # Equation 2
    param_block = np.round(param_block) #Rounding the possible block sizes s
    quantized_width = initial_width * np.power(quantized_param, param_block) # Equation 3
    quantized_width = np.round(quantized_width / 8) * 8 # Make all the width list divisible by 8
    quantized_width, quantized_depth = np.unique(quantized_width.astype(np.int), return_counts=True) #Finding depth and width lists.
    gtemp = np.minimum(group_width, quantized_width//bottleneck_ratio)
    quantized_width = (np.round(quantized_width // bottleneck_ratio / gtemp) * gtemp).astype(int) #To make all the width compatible with group sizes of the 3x3 convolutional layers
    group_width = np.unique(gtemp // bottleneck_ratio)[0]
    #model parameter intialization
    self.in_filters = in_filters
    self.quantized_width=quantized_width
    self.quantized_depth=quantized_depth
    self.bottleneck_ratio=bottleneck_ratio
    self.group_width=group_width
    self.se_ratio=se_ratio
    self.num_layers=number_layer
    #print(self.quantized_width)
    #print(self.quantized_depth)
    #print(self.bottleneck_ratio)
    #print(self.group_width)
    #print(self.se_ratio)
    #print(self.num_layers)
    #Stem architecture
    self.stem = Stem(self.in_filters)
    self.body = []
    for index in range(self.num_layers):
      if index==0:
        layer=Stage(self.in_filters, self.quantized_depth[index], self.quantized_width[index], self.bottleneck_ratio, self.group_width, self.se_ratio,stride_flag=1)
      else:
        layer=Stage(self.in_filters, self.quantized_depth[index], self.quantized_width[index], self.bottleneck_ratio, self.group_width, self.se_ratio,stride_flag=0)
      self.body.append(layer)
      self.in_filters = self.quantized_width[index]
    #Body:Four layers containing bottleneck residual blocks
    self.body = nn.Sequential(*self.body)
    #Head: Classification Step FC + AveragePool
    self.head = Head(self.quantized_width[-1], classes)
  def forward(self, x):
    out = self.stem(x)
    out = self.body(out)
    out = self.head(out)
    return out



#TRAINING (linear warm up with cosine decay learning rate)
TRAINING_LR_MAX          = 0.001
TRAINING_LR_INIT_SCALE   = 0.01
TRAINING_LR_INIT_EPOCHS  = 5
TRAINING_LR_FINAL_SCALE  = 0.01
TRAINING_LR_FINAL_EPOCHS = 55
# TRAINING_LR_FINAL_EPOCHS = 2 # uncomment for a quick test
TRAINING_NUM_EPOCHS      = TRAINING_LR_INIT_EPOCHS + TRAINING_LR_FINAL_EPOCHS
TRAINING_LR_INIT         = TRAINING_LR_MAX*TRAINING_LR_INIT_SCALE
TRAINING_LR_FINAL        = TRAINING_LR_MAX*TRAINING_LR_FINAL_SCALE
FILE_SAVE = 0
FILE_LOAD = 0
FILE_NAME = 'RegNetX200.pt'

# learning rate schedule #implemented two different decay
def adjust_learning_rate(optimizer, epoch, learning_rate, type="cosine"):
    if type=="cosine_decay":
      if epoch < TRAINING_LR_INIT_EPOCHS:
          lr = (TRAINING_LR_MAX - TRAINING_LR_INIT)*(float(epoch)/TRAINING_LR_INIT_EPOCHS) + TRAINING_LR_INIT
      else:
          lr = (TRAINING_LR_MAX - TRAINING_LR_FINAL)*max(0.0, math.cos(((float(epoch) - TRAINING_LR_INIT_EPOCHS)/(TRAINING_LR_FINAL_EPOCHS - 1.0))*(math.pi/2.0))) + TRAINING_LR_FINAL
      for param_group in optimizer.param_groups:
          param_group['lr'] = lr
    
    if type=="time_decay":
      lr = learning_rate * (0.1 ** (epoch // 30))
      for param_group in optimizer.param_groups:
        param_group['lr'] = lr
    return lr
    

from torch import nn
from torch.optim import SGD
from torch.utils.tensorboard import SummaryWriter


if __name__ == '__main__':
  #model parameters
  network_depth=13
  initial_width=24
  slope=36
  quantized_param=2.5
  bottleneck_ratio=1
  group_width=8
  se_ratio=1
  in_filters=32
  num_layers=4
  start_epoch=0

  #training parameters
  learning_rate=TRAINING_LR_INIT_SCALE
  momentum=0.9
  weight_decay=5e-4
  epochs=TRAINING_NUM_EPOCHS
  batch_size=512
  device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

  model = RegNet(network_depth,initial_width,in_filters,num_layers,slope,quantized_param,bottleneck_ratio,group_width,se_ratio)
  print(model)
  if torch.cuda.is_available():
        model = nn.DataParallel(model)
        model = model.to(device)

  if FILE_LOAD == 1:
    checkpoint = torch.load(FILE_NAME)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    start_epoch = checkpoint['epoch'] + 1
  
  criterion = nn.CrossEntropyLoss()
  #optimizer = SGD(model.parameters(),lr=learning_rate,momentum=momentum,weight_decay=weight_decay, nesterov=True)
  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
  best_acc1 = 0
  model.train()
  tlist=[]
  alist=[]
  for epoch in range(start_epoch, TRAINING_NUM_EPOCHS):
    start=time.time()
    model.train()
    training_loss = 0.0
    num_batches   = 0
    # set the learning rate for the epoch
    learning_rate=adjust_learning_rate(optimizer, epoch, learning_rate,type="time_decay")
    # cycle through the train set
    for data in dataloader_train:
        # extract a batch of data and move it to the appropriate device
        inputs, labels = data
        inputs, labels = inputs.to(device), labels.to(device)
        # zero the parameter gradients
        optimizer.zero_grad()
        # forward pass, loss, backward pass and weight update
        outputs = model(inputs)
        loss    = criterion(outputs, labels)
        loss.backward()
        optimizer.step()
        # update statistics
        training_loss = training_loss + loss.item()
        num_batches   = num_batches + 1
    # initialize test set statistics
    model.eval()
    test_correct = 0
    test_total   = 0
    # no weight update / no gradient needed
    with torch.no_grad():
        # cycle through the test set
        for data in dataloader_test:
            # extract a batch of data and move it to the appropriate device
            inputs, labels = data
            inputs, labels = inputs.to(device), labels.to(device)
            # forward pass and prediction
            outputs      = model(inputs)
            _, predicted = torch.max(outputs.data, 1)
            # update test set statistics
            test_total   = test_total + labels.size(0)
            test_correct = test_correct + (predicted == labels).sum().item()
    # epoch statistics
    end = float(time.time()-start)
    print('Epoch {0:2d} lr = {1:8.6f} avg loss = {2:8.6f} accuracy = {3:5.2f} time={4:5.2f}'.format(epoch, learning_rate, (training_loss/num_batches)/DATA_BATCH_SIZE, (100.0*test_correct/test_total),end))
    tlist.append((training_loss/num_batches)/DATA_BATCH_SIZE)
    alist.append((100.0*test_correct/test_total))
    
    # model loading
    if FILE_SAVE == 1:
      torch.save({
          'epoch': TRAINING_NUM_EPOCHS - 1,
          'model_state_dict': model.state_dict(),
          'optimizer_state_dict': optimizer.state_dict()
      }, FILE_NAME)

  plt.plot(tlist)
  plt.title("Training loss Vs  Epoch")
  plt.xlabel("Epoch")
  plt.ylabel("Loss") 
  plt.show()

  plt.plot(alist)
  plt.title("Testing Accuracy Vs Epoch")
  plt.xlabel("Epoch")
  plt.ylabel("Testing Accuracy") 
  plt.show()

###############################################################################
#
# TEST
#
################################################################################
  # initialize test set statistics
  model.eval()
  test_correct=0
  test_total=0
  # initialize class statistics
  class_correct = list(0. for i in range(DATA_NUM_CLASSES))
  class_total   = list(0. for i in range(DATA_NUM_CLASSES))
  # no weight update / no gradient needed
  with torch.no_grad():
      # cycle through the test set
      for data in dataloader_test:
          # extract a batch of data and move it to the appropriate device
          inputs,labels = data
          inputs,labels = inputs.to(device), labels.to(device)
          # forward pass and prediction
          outputs= model(inputs)
          _, predicted=torch.max(outputs.data, 1)
          # update test set statistics
          test_total   = test_total + labels.size(0)
          test_correct = test_correct + (predicted == labels).sum().item()
          # update class statistics
          c = (predicted == labels).squeeze()
          for i in range(labels.size(0)):
              label=labels[i]
              class_correct[label]+=c[i].item()
              class_total[label]+= 1
  # test set statistics
  print('Final accuracy of test set = {0:5.2f}'.format((100.0*test_correct/test_total)))
  print('')



